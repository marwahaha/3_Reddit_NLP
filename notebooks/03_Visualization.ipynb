{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "This notebook will contain my visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "import imageio\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in\n",
    "\n",
    "sub_list = ['mc','fb','mls','ssfc','tfr','tfys', 'bar']\n",
    "\n",
    "for sub in sub_list:\n",
    "    if sub == 'mc':\n",
    "        csvfilename = '../datasets/motorcycles.csv'\n",
    "    elif sub == 'fb':\n",
    "        csvfilename = '../datasets/soccer.csv'\n",
    "    elif sub == 'mls':\n",
    "        csvfilename = '../datasets/mls.csv'\n",
    "    elif sub == 'ssfc':\n",
    "        csvfilename = '../datasets/sounders.csv'\n",
    "    elif sub == 'tfr':\n",
    "        csvfilename = '../datasets/tfr.csv'\n",
    "    elif sub == 'bar':\n",
    "        csvfilename = '../datasets/bar.csv'\n",
    "    else:\n",
    "        csvfilename = '../datasets/tfys.csv'\n",
    "        \n",
    "# Read the CSV in, drop the Unnamed column, drop the first index, which is blank, reset the index\n",
    "    df = pd.read_csv(csvfilename)\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    df.drop(index=0, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# lowercase everything but the author\n",
    "    df['title'] = df.title.str.lower()\n",
    "    df['text'] = df.text.str.lower()\n",
    "    \n",
    "# if the text, the actual post, is Nan, replace with spaces. Spaces won't affect the process\n",
    "    df['text'].fillna(' ', inplace=True)\n",
    "        \n",
    "        \n",
    "# put the data into the correct dataframe, one per sub, so facilitate analysis\n",
    "    if sub == 'mc':\n",
    "        mc_df = df\n",
    "    elif sub == 'fb':\n",
    "        fb_df = df\n",
    "    elif sub == 'mls':\n",
    "        mls_df = df\n",
    "    elif sub == 'tfr':\n",
    "        tfr_df = df\n",
    "    elif sub == 'tfys':\n",
    "        tfys_df = df\n",
    "    elif sub == 'bar':\n",
    "        bar_df = df\n",
    "    else:\n",
    "        ssfc_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take in a dirty pd series and return a clean corpus with no numbers or punctuation\n",
    "\n",
    "def string_clean(series):\n",
    "\n",
    "# init the corpus\n",
    "    corpus = []\n",
    "    \n",
    "    \n",
    "# Step throught the series\n",
    "    for i, val in series.iteritems():\n",
    "        \n",
    "# Create a list of all the clean words/tokens\n",
    "        clean_list = re.findall(r'\\b[^\\d\\W]+\\b', val)\n",
    "\n",
    "# Create the clean string with the tokens separated by spaces\n",
    "        s_clean = ' '.join(clean_list)\n",
    "    \n",
    "# Append to the corpus\n",
    "        corpus.append(s_clean)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the vocabularies of the sent corpora. Returns three lists. One each of all the words \n",
    "# in the given corpus and a list of the words that exist in both.\n",
    "# The other point of the function is to use the word corpora.\n",
    "\n",
    "def vocab_lists(corp1, corp2):\n",
    "    \n",
    "    X_1 = string_clean(corp1)\n",
    "    X_2 = string_clean(corp2)\n",
    "    \n",
    "    cvec = CountVectorizer(stop_words='english', max_features=8000)\n",
    "    \n",
    "    cvec.fit_transform(X_1)\n",
    "    vocab1 = cvec.get_feature_names()\n",
    "\n",
    "    cvec.fit_transform(X_2)\n",
    "    vocab2 = cvec.get_feature_names()\n",
    "    \n",
    "    join = []\n",
    "    for word in vocab1:\n",
    "        if word in vocab2:\n",
    "            join.append(word)\n",
    "            \n",
    "\n",
    "    \n",
    "    return vocab1, vocab2, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary size and joint vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mc = mc_df['title']\n",
    "fb = fb_df['title']\n",
    "\n",
    "mc_v, fb_v, join = vocab_lists(mc,fb)\n",
    "print(f'mc len: {len(mc_v)}')\n",
    "print(f'fb len: {len(fb_v)}')\n",
    "print(f'joint len: {len(join)}')\n",
    "print(f'mc total len: {mc_df.shape[0]}')\n",
    "print(f'fb total len: {fb_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mls = mls_df['title']\n",
    "fb = fb_df['title']\n",
    "\n",
    "mls_v, fb_v, join = vocab_lists(mls,fb)\n",
    "print(f'mls len: {len(mls_v)}')\n",
    "print(f'fb len: {len(fb_v)}')\n",
    "print(f'joint len: {len(join)}')\n",
    "print(f'fb total len: {fb_df.shape[0]}')\n",
    "print(f'mls total len: {mls_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mls = mls_df['title']\n",
    "ssfc = ssfc_df['title']\n",
    "\n",
    "mls_v, ssfc_v, join = vocab_lists(mls,ssfc)\n",
    "print(f'mls len: {len(mls_v)}')\n",
    "print(f'ssfc len: {len(ssfc_v)}')\n",
    "print(f'joint len: {len(join)}')\n",
    "print(f'mls total len: {mls_df.shape[0]}')\n",
    "print(f'ssfc total len: {ssfc_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr = tfr_df['title']\n",
    "tfys = tfys_df['title']\n",
    "\n",
    "tfr_v, tfys_v, join = vocab_lists(tfr,tfys)\n",
    "print(f'tfr len: {len(tfr_v)}')\n",
    "print(f'tfys len: {len(tfys_v)}')\n",
    "print(f'joint len: {len(join)}')\n",
    "print(f'tfr total len: {tfr_df.shape[0]}')\n",
    "print(f'tfys total len: {tfys_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfys = tfys_df['title']\n",
    "bar = bar_df['title']\n",
    "\n",
    "tfr_v, bar_v, join = vocab_lists(tfr,bar)\n",
    "print(f'tfys len: {len(tfys_v)}')\n",
    "print(f'bar len: {len(bar_v)}')\n",
    "print(f'joint len: {len(join)}')\n",
    "print(f'tfys total len: {tfys_df.shape[0]}')\n",
    "print(f'bar total len: {bar_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfys = pd.Series(tfys_df['title'] + ' ' + tfys_df['text'])\n",
    "bar = pd.Series(bar_df['title'] + ' ' + bar_df['text'])\n",
    "\n",
    "tfr_v, bar_v, join = vocab_lists(tfr,bar)\n",
    "print(f'tfys len: {len(tfys_v)}')\n",
    "print(f'bar len: {len(bar_v)}')\n",
    "print(f'joint len: {len(join)}')\n",
    "print(f'tfys total len: {tfys_df.shape[0]}')\n",
    "print(f'bar total len: {bar_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word clouds of happiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shamelessly stolen from Mr. Charles Rice.\n",
    "# Setting up stopwords for future use\n",
    "stop = stopwords.words('english')\n",
    "punct = {'\"', \"'\", '.', ',', '-', '--', '!', ';', '?', ':', '(', ')', '``', \"''\", '``'}\n",
    "stop = {x.lower() for x in stop}\n",
    "stop = stop|punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in mc_v:\n",
    "    mc_str = ' '.join(mc_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also from Charlie. Thanks!\n",
    "# Motorcycle Word Cloud\n",
    "wc = WordCloud(width=800, height=400, stopwords=stop)\n",
    "\n",
    "wc.generate_from_text(mc_str)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('../assets/motorcycle_wc.jpg', quality=95, dpi = 250)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in fb_v:\n",
    "    fb_str = ' '.join(fb_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also from Charlie. Thanks!\n",
    "# Football (soccer for you Americans) Word Cloud\n",
    "wc = WordCloud(width=800, height=400, stopwords=stop)\n",
    "\n",
    "wc.generate_from_text(fb_str)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('../assets/football_wc.jpg', quality=95, dpi=250)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud with a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_mask = imageio.imread('../assets/motorcycle_mask.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black', stopwords=STOPWORDS, mask=bike_mask, collocations=False)\n",
    "\n",
    "wc.generate_from_text(mc_str)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.savefig('../assets/motorcyclemask_wc.jpg', quality=95, dpi=250)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ball_mask = imageio.imread('../assets/football_mask.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='black', mode='RGBA', stopwords=STOPWORDS, mask=ball_mask, collocations=False)\n",
    "\n",
    "wc.generate_from_text(fb_str)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.savefig('../assets/footballmask_wc.jpg', quality=95, dpi=250)\n",
    "plt.figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
