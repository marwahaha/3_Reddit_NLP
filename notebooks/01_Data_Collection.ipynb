{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "This notebook will step through the process of gathering data from Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required headers\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "`get_reddit` will get just over 1000 posts from the given sub, and return a list of the json data\n",
    "\n",
    "`write_reddit` will step through the json and pull the relevant data out, and store it a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of posts from the reddit subs\n",
    "def get_reddit(url):\n",
    "\n",
    "# Initialize json holder\n",
    "    posts = []\n",
    "\n",
    "# Initialize after param\n",
    "    after = None\n",
    "    \n",
    "# Create a custom header\n",
    "    headers = {'User-agent': 'Elliott 1.0'}\n",
    "\n",
    "# Get ~1000 posts\n",
    "    for i in range(50):\n",
    "        if i % 5 == 0:\n",
    "            print(f'{i} posts have been collected.')\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "        \n",
    "        res = requests.get(url, params=params, headers=headers)\n",
    "        if res.status_code == 200:\n",
    "            json = res.json()\n",
    "            posts.extend(json['data']['children'])\n",
    "            after = json['data']['after'] \n",
    "        else:\n",
    "            print(f'Error! Status code: {res.status_code}')\n",
    "        break \n",
    "              \n",
    "        time.sleep(2)\n",
    "        \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data into a dataframe and then into a csv file\n",
    "\n",
    "def write_reddit(posts, csvfilename):\n",
    "\n",
    "# Initialize our corpus\n",
    "    corpus = [[]]\n",
    "\n",
    "# Step through all the posts, pulling the relevant data.\n",
    "# Data is stored in a list, or document, which is then appended onto the corpus\n",
    "    for i in range(len(posts)):\n",
    "        document = []\n",
    "        document.append(posts[i]['data']['title'])\n",
    "        document.append(posts[i]['data']['selftext'])\n",
    "        document.append(posts[i]['data']['author'])\n",
    "        document.append(posts[i]['data']['subreddit_name_prefixed'])\n",
    "    \n",
    "        corpus.append(document)\n",
    "    \n",
    "    \n",
    "# Convert to a dataframe to allow easy CSV writting    \n",
    "    df = pd.DataFrame(corpus, columns=['title','text','author','sub'])\n",
    "\n",
    "# change the sub to something shorter\n",
    "    df['sub'] = df['sub'].map(\n",
    "              {'r/motorcycles':'mc',\n",
    "               'r/MLS': 'mls',\n",
    "               'r/soccer':'fb',\n",
    "               'r/SoundersFC':'ssfc',\n",
    "               'r/TalesFromRetail':'tfr',\n",
    "               'r/TalesFromYourServer':'tfys',\n",
    "               'r/bartenders':'bar'})\n",
    "        \n",
    "    \n",
    "# Drop all the duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "# Save that file off to disk!\n",
    "    df.to_csv(csvfilename)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "Step through the list of subreddits I want to use. Grab the posts and then shove them into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the subs I'm going to hit\n",
    "sub_list = ['mc','fb','mls','ssfc','tfr','tfys', 'bar']\n",
    "\n",
    "for sub in sub_list:\n",
    "    if sub == 'mc':\n",
    "        csvfilename = '../datasets/motorcycles.csv'\n",
    "        url = 'https://www.reddit.com/r/motorcycles.json'\n",
    "    elif sub == 'fb':\n",
    "        csvfilename = '../datasets/soccer.csv'\n",
    "        url = 'https://www.reddit.com/r/soccer.json'\n",
    "    elif sub == 'mls':\n",
    "        csvfilename = './datasets/mls.csv'\n",
    "        url = 'https://www.reddit.com/r/mls.json'\n",
    "    elif sub == 'ssfc':\n",
    "        csvfilename = '../datasets/sounders.csv'\n",
    "        url = 'https://www.reddit.com/r/SoundersFC.json'\n",
    "    elif sub == 'tfr':\n",
    "        csvfilename = '../datasets/tfr.csv'\n",
    "        url = 'https://www.reddit.com/r/TalesFromRetail.json'\n",
    "    elif sub == 'bar':\n",
    "        csvfilename = '../datasets/bar.csv'\n",
    "        url = 'https://www.reddit.com/r/bartenders.json'\n",
    "    else:\n",
    "        csvfilename = '../datasets/tfys.csv'\n",
    "        url = 'https://www.reddit.com/r/TalesFromYourServer.json'\n",
    "        \n",
    "# Get the data from reddit\n",
    "    posts = get_reddit(url)\n",
    "    \n",
    "# Write the data to csv\n",
    "    write_reddit(posts, csvfilename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
