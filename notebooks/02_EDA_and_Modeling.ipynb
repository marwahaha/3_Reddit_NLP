{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Modeling\n",
    "\n",
    "\n",
    "This notebook will step through the collected data and prepare it for modeling.\n",
    "\n",
    "I attempted to create a function that took a dirty string in, and returned a tokenized, clean string. This worked for a single string, but I could not get the function to work in a dataframe.map() context.\n",
    "\n",
    "Since I have to pull the string out of the dataframe to put it into a matrix for vectorizing, I'll just do the cleaning then.\n",
    "\n",
    "\n",
    "### Contents\n",
    "- [Data Load](#Data-Load)\n",
    "- [r/motorcycle vs. r/soccer](#model-1)\n",
    "- [r/soccer vs. r/MLS](#model-2)  \n",
    "- [r/MLS vs. r/SoundersFC](#model-3)\n",
    "- [Multi-class Regression](#model-4)\n",
    "- [*k*-Nearest Neighbors](#model-5)\n",
    "- [r/TalesFromRetail vs. r/TalesFromYourServer ](#model-6)\n",
    "- [r/TalesFromYourServer vs. r/bartenders](#model-7)\n",
    "- [GridSearchCV() on two different models](#model-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import stop_words, text\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load\n",
    "\n",
    "Load each of the subreddits' data into a dataframe and make a first cleaning pass using the built-in string methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = ['mc','fb','mls','ssfc','tfr','tfys', 'bar']\n",
    "\n",
    "for sub in sub_list:\n",
    "    if sub == 'mc':\n",
    "        csvfilename = '../datasets/motorcycles.csv'\n",
    "    elif sub == 'fb':\n",
    "        csvfilename = '../datasets/soccer.csv'\n",
    "    elif sub == 'mls':\n",
    "        csvfilename = '../datasets/mls.csv'\n",
    "    elif sub == 'ssfc':\n",
    "        csvfilename = '../datasets/sounders.csv'\n",
    "    elif sub == 'tfr':\n",
    "        csvfilename = '../datasets/tfr.csv'\n",
    "    elif sub == 'bar':\n",
    "        csvfilename = '../datasets/bar.csv'\n",
    "    else:\n",
    "        csvfilename = '../datasets/tfys.csv'\n",
    "        \n",
    "# Read the CSV in, drop the Unnamed column, drop the first index, which is blank, reset the index\n",
    "    df = pd.read_csv(csvfilename)\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    df.drop(index=0, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# lowercase everything but the author\n",
    "    df['title'] = df.title.str.lower()\n",
    "    df['text'] = df.text.str.lower()\n",
    "    \n",
    "# if the text, the actual post, is Nan, replace with spaces. Spaces won't affect the process\n",
    "    df['text'].fillna(' ', inplace=True)\n",
    "        \n",
    "        \n",
    "# put the data into the correct dataframe, one per sub, to facilitate analysis\n",
    "    if sub == 'mc':\n",
    "        mc_df = df\n",
    "    elif sub == 'fb':\n",
    "        fb_df = df\n",
    "    elif sub == 'mls':\n",
    "        mls_df = df\n",
    "    elif sub == 'tfr':\n",
    "        tfr_df = df\n",
    "    elif sub == 'tfys':\n",
    "        tfys_df = df\n",
    "    elif sub == 'bar':\n",
    "        bar_df = df\n",
    "    else:\n",
    "        ssfc_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "I created two functions to streamline the code, since I was doing many of the same things over and over. Sort of the whole point of a function.\n",
    "\n",
    "#### `string_clean(series)`\n",
    "This takes a dirty series of documents, cleans and tokenizes, and returns a clean corpus\n",
    "\n",
    "\n",
    "#### `vector_scores(X,y,steps)`\n",
    "This will take a clean X, a clean y, and a list of steps for a pipeline\n",
    "\n",
    "The data is split then run through a series of pipelines. Each pipeline contains at least a vectorizer and a logistic regression object. The pipelines are fit, transformed, and scored with the results being stored in a dataframe, which is returned. A confusion matrix is also created using the test data; this is returned as well. The metrics for a binary classification are also computed and returned as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take in a dirty pd series and return a clean corpus with no numbers or punctuation\n",
    "\n",
    "def string_clean(series):\n",
    "\n",
    "# init the corpus\n",
    "    corpus = []\n",
    "    \n",
    "    \n",
    "# Step throught the series\n",
    "    for i, val in series.iteritems():\n",
    "        \n",
    "# Create a list of all the clean words/tokens\n",
    "        clean_list = re.findall(r'\\b[^\\d\\W]+\\b', val)\n",
    "\n",
    "# Create the clean string with the tokens separated by spaces\n",
    "        s_clean = ' '.join(clean_list)\n",
    "    \n",
    "# Append to the corpus\n",
    "        corpus.append(s_clean)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take a clean X and y, run it through three different vectorizing methods and then \n",
    "# return the scores as a data frame. Also takes in a list of steps, and assumes the vectorizer is first step\n",
    "# return the confusion matrix for the test data. The metrics are calculated for binary regression and returned\n",
    "# as a datafram\n",
    "\n",
    "def vector_scores(X, y, steps):\n",
    "    \n",
    "# split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "    \n",
    "#  Create a list of the vectors we are going to use in the steps of the pipeline as well as a list for\n",
    "#  the dataframe\n",
    "    vectors = [('cvec',CountVectorizer(stop_words='english')),\n",
    "               ('tvec',TfidfVectorizer(stop_words='english')),\n",
    "               ('hvec',HashingVectorizer(stop_words='english'))]\n",
    "    vector_list = ['CountVector','TfidfVector','HashVector']\n",
    "\n",
    "# initilize list iterater\n",
    "    i=0\n",
    "\n",
    "# init holding lists\n",
    "    vec_list = []\n",
    "    set_list = []\n",
    "    score_list = []\n",
    "\n",
    "    for vec in vectors:\n",
    "    \n",
    "# Set the pipeline step with the correct vector\n",
    "        steps[0] = vec\n",
    "        pipe = Pipeline(steps)\n",
    "        \n",
    "    \n",
    "# Fit Train\n",
    "        pipe.fit(X_train, y_train)\n",
    "    \n",
    "# Score Train\n",
    "        vec_list.append(vector_list[i])\n",
    "        set_list.append('train')\n",
    "        score_list.append(pipe.score(X_train, y_train))\n",
    "                      \n",
    "# Score Test\n",
    "        vec_list.append(vector_list[i])\n",
    "        set_list.append('test')\n",
    "        score_list.append(pipe.score(X_test, y_test)) \n",
    "    \n",
    "        i = i + 1\n",
    "                      \n",
    "                  \n",
    "# Fill scores dataframe\n",
    "    scores_df = pd.DataFrame(columns = ['vector','set','score'])                      \n",
    "    scores_df['vector'] = vec_list\n",
    "    scores_df['set'] = set_list\n",
    "    scores_df['score'] = score_list\n",
    "    \n",
    "# Reset to CountVectorizer(), as that's what I'm using for MultinomialNB\n",
    "    steps = [('cvec',CountVectorizer(stop_words='english')),\n",
    "             ('lr', LogisticRegression(solver='liblinear', multi_class='auto'))]\n",
    "    pipe = Pipeline(steps)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "# Create the confusion matrix\n",
    "# Predict Test\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "# Confusion Matrix\n",
    "    cf = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "# Create a dataframe of metrics. If there are more than two classes, send an empty dataframe\n",
    "    try:\n",
    "        tn, fp, fn, tp = cf.ravel()\n",
    "        n = tn+fp+fn+tp\n",
    "        accuracy = (tp+tn)/n\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        precision = tp/(tp+fp)\n",
    "        specificity = tn/(tn+fp)\n",
    "        missclass = (fp+fn)/n\n",
    "        value_list = [accuracy, missclass, precision, sensitivity, specificity]\n",
    "        metric_df = pd.DataFrame(columns=['metric', 'value'])\n",
    "        metric_df.metric = ['Accuracy','Missclassification','Precision','Sensitivity','Specificity']\n",
    "        metric_df.value = value_list\n",
    "    except:\n",
    "        metric_df = [[]]\n",
    "        \n",
    "        \n",
    "    return scores_df, cf, metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-2'></a>\n",
    "\n",
    "## r/motorcycle vs. r/soccer\n",
    "\n",
    "For this model I will look at bikes vs. balls.\n",
    "\n",
    "Can I reliably discern where the post came from, r/motorcyce or r/soccer?\n",
    "\n",
    "I will also use the CountVectorizer(), TFIDFVectorizer(), and HashVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from r/motorcycle and r/soccer\n",
    "data1_df = pd.concat([mc_df, fb_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_mc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the steps\n",
    "steps = [\n",
    "    ('hold', 'hold'),\n",
    "    ('lr', LogisticRegression(solver='liblinear', multi_class='auto' ))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df, cf, metric_df = vector_scores(X,y,steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do multinomial naive bayes. I had tried to include this in the function, but some odd error that I couldn't solve cropped up. So I'm doing a pipeline of NB Multi, using the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from r/motorcycle and r/soccer\n",
    "data1_df = pd.concat([mc_df, fb_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_mc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "# create the steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('mnb', MultinomialNB())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# init holding lists\n",
    "vec_list = []\n",
    "set_list = []\n",
    "score_list = []\n",
    "    \n",
    "# Fit Train\n",
    "pipe.fit(X_train, y_train)\n",
    "    \n",
    "# Score Train\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('train')\n",
    "score_list.append(pipe.score(X_train, y_train))\n",
    "                      \n",
    "# Score Test\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('test')\n",
    "score_list.append(pipe.score(X_test, y_test)) \n",
    "                 \n",
    "# Fill scores dataframe\n",
    "scores_df = pd.DataFrame(columns = ['vector','set','score'])                      \n",
    "scores_df['vector'] = vec_list\n",
    "scores_df['set'] = set_list\n",
    "scores_df['score'] = score_list\n",
    "    \n",
    "# Create the confusion matrix\n",
    "# Predict Test\n",
    "y_pred = pipe.predict(X_test)\n",
    "    \n",
    "# Confusion Matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = cf.ravel()\n",
    "n = tn+fp+fn+tp\n",
    "accuracy = (tp+tn)/n\n",
    "sensitivity = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "specificity = tn/(tn+fp)\n",
    "missclass = (fp+fn)/n\n",
    "value_list = [accuracy, missclass, precision, sensitivity, specificity]\n",
    "metric_df = pd.DataFrame(columns=['metric', 'value'])\n",
    "metric_df.metric = ['Accuracy','Missclassification','Precision','Sensitivity','Specificity']\n",
    "metric_df.value = value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train had a 1.0 score?!?!?!?!\n",
    "\n",
    "Let's go check that out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from r/motorcycle and r/soccer\n",
    "data1_df = pd.concat([mc_df, fb_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_mc']\n",
    "\n",
    "# Get the scores\n",
    "# scores_df = vector_scores(X,y, steps)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "\n",
    "mc_stop_words = ['motorcycle','motorcycles']\n",
    "\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(mc_stop_words)\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "\n",
    "X_train_c = cvec.fit_transform(X_train)\n",
    "X_test_c = cvec.transform(X_test)\n",
    "\n",
    "lr.fit(X_train_c, y_train)\n",
    "\n",
    "train_score = lr.score(X_train_c, y_train)\n",
    "test_score = lr.score(X_test_c, y_test)\n",
    "print(f'Train: {train_score}')\n",
    "print(f'Test: {test_score}')\n",
    "print(f'Cross: {cross_val_score(lr, X_train_c, y_train, cv=5).mean()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features and their coefficents to look at\n",
    "c_df = pd.DataFrame(lr.coef_, columns=cvec.get_feature_names()).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = c_df.sort_values(by=0, ascending=False).head(15)\n",
    "bottom = c_df.sort_values(by=0, ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.rename(columns={0:'Coef'}, inplace=True)\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom.rename(columns={0:'Coef'}, inplace=True)\n",
    "bottom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "I find it very interesting that there are so many words so heavily weighted. Makes a bit more sense why train is overfitting so badly. Also, this is why I went and got several other subs of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Total number of positive terms >= 1\n",
    "\n",
    "pos = c_df[ c_df[0] >= 1].sort_values(by=0, ascending=False)\n",
    "print(f'Mean of positive terms: {pos[0].mean()}')\n",
    "print(f'Shape: {pos.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of negative terms <= -1\n",
    "\n",
    "neg = c_df[ c_df[0] <= -1].sort_values(by=0, ascending=True)\n",
    "print(f'Mean of negative eterms: {neg[0].mean()}')\n",
    "print(f'Shape: {neg.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-1'></a>\n",
    "\n",
    "## r/soccer vs. r/mls\n",
    "\n",
    "Since motorcycles and the language pertaining thereto aren't very similar to soccer-sports-ball, lets compare two more closely related subs to see what the results are. Namely, we'll look at the world-wide (although in practice fairly Euro-centric) r/soccer and the league specific r/mls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([fb_df, mls_df])\n",
    "\n",
    "# hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "# creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_mls']\n",
    "\n",
    "# Create the steps\n",
    "steps = [\n",
    "    ('hold', 'hold'),\n",
    "    ('lr', LogisticRegression(solver='liblinear', multi_class='auto' ))]\n",
    "\n",
    "# Get the scores\n",
    "scores_df, cf, metric_df = vector_scores(X,y,steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaiveBayes Multi with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([fb_df, mls_df])\n",
    "\n",
    "# hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "# creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_mls']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "# create the steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('mnb', MultinomialNB())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# init holding lists\n",
    "vec_list = []\n",
    "set_list = []\n",
    "score_list = []\n",
    "    \n",
    "# Fit Train\n",
    "pipe.fit(X_train, y_train)\n",
    "    \n",
    "# Score Train\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('train')\n",
    "score_list.append(pipe.score(X_train, y_train))\n",
    "                      \n",
    "# Score Test\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('test')\n",
    "score_list.append(pipe.score(X_test, y_test)) \n",
    "                 \n",
    "# Fill scores dataframe\n",
    "scores_df = pd.DataFrame(columns = ['vector','set','score'])                      \n",
    "scores_df['vector'] = vec_list\n",
    "scores_df['set'] = set_list\n",
    "scores_df['score'] = score_list\n",
    "    \n",
    "# Create the confusion matrix\n",
    "# Predict Test\n",
    "y_pred = pipe.predict(X_test)\n",
    "    \n",
    "# Confusion Matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = cf.ravel()\n",
    "n = tn+fp+fn+tp\n",
    "accuracy = (tp+tn)/n\n",
    "sensitivity = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "specificity = tn/(tn+fp)\n",
    "missclass = (fp+fn)/n\n",
    "value_list = [accuracy, missclass, precision, sensitivity, specificity]\n",
    "metric_df = pd.DataFrame(columns=['metric', 'value'])\n",
    "metric_df.metric = ['Accuracy','Missclassification','Precision','Sensitivity','Specificity']\n",
    "metric_df.value = value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis.\n",
    "This was, as expected, not perfect. Much more cross-over in terms of specific vocabulary.\n",
    "\n",
    "Let's do one more test: a team specific sub vs. the league in which it plays. r/SounderFC vs r/MLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-3'></a>\n",
    "\n",
    "## r/MLS vs. r/SoundersFC\n",
    "\n",
    "Further down the rabbit hole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([mls_df, ssfc_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_ssfc']\n",
    "\n",
    "# Create the steps\n",
    "steps = [\n",
    "    ('hold', 'hold'),\n",
    "    ('lr', LogisticRegression(solver='liblinear', multi_class='auto' ))]\n",
    "\n",
    "# Get the scores\n",
    "scores_df, cf, metric_df = vector_scores(X,y,steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaiveBayes Multi with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([mls_df, ssfc_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_ssfc']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "# create the steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('mnb', MultinomialNB())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# init holding lists\n",
    "vec_list = []\n",
    "set_list = []\n",
    "score_list = []\n",
    "    \n",
    "# Fit Train\n",
    "pipe.fit(X_train, y_train)\n",
    "    \n",
    "# Score Train\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('train')\n",
    "score_list.append(pipe.score(X_train, y_train))\n",
    "                      \n",
    "# Score Test\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('test')\n",
    "score_list.append(pipe.score(X_test, y_test)) \n",
    "                 \n",
    "# Fill scores dataframe\n",
    "scores_df = pd.DataFrame(columns = ['vector','set','score'])                      \n",
    "scores_df['vector'] = vec_list\n",
    "scores_df['set'] = set_list\n",
    "scores_df['score'] = score_list\n",
    "    \n",
    "# Create the confusion matrix\n",
    "# Predict Test\n",
    "y_pred = pipe.predict(X_test)\n",
    "    \n",
    "# Confusion Matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = cf.ravel()\n",
    "n = tn+fp+fn+tp\n",
    "accuracy = (tp+tn)/n\n",
    "sensitivity = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "specificity = tn/(tn+fp)\n",
    "missclass = (fp+fn)/n\n",
    "value_list = [accuracy, missclass, precision, sensitivity, specificity]\n",
    "metric_df = pd.DataFrame(columns=['metric', 'value'])\n",
    "metric_df.metric = ['Accuracy','Missclassification','Precision','Sensitivity','Specificity']\n",
    "metric_df.value = value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis.\n",
    "It's a bit over-fit, but not much. I am honestly surprised at this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-4'></a>\n",
    "\n",
    "## Multi-class regression\n",
    "\n",
    "EVERYTHING!!!!!!!\n",
    "\n",
    "Seriously, I'm going to do a multi-class regression against the whole shebang. And then I'll cook an egg on my poor laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([mc_df, fb_df, mls_df, ssfc_df])\n",
    "\n",
    "# instead of one-hot encoding the target apply a map to the 'sub'\n",
    "sub_map = {'mc': 0,\n",
    "           'fb': 1,\n",
    "           'mls': 2,\n",
    "           'ssfc':3}\n",
    "            \n",
    "data1_df['sub'] = data1_df['sub'].map(sub_map).astype(int)\n",
    "\n",
    "#create our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub']\n",
    "\n",
    "# Create the steps\n",
    "steps = [\n",
    "    ('hold', 'hold'),\n",
    "    ('lr', LogisticRegression(solver='liblinear', multi_class='auto' ))]\n",
    "\n",
    "# Get the scores\n",
    "scores_df, cf, metric_df = vector_scores(X,y,steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['mc','fb','mls','ssfc'], index=['mc','fb','mls','ssfc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaiveBayes Multi with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([mc_df, fb_df, mls_df, ssfc_df])\n",
    "\n",
    "# instead of one-hot encoding the target apply a map to the 'sub'\n",
    "sub_map = {'mc': 0,\n",
    "           'fb': 1,\n",
    "           'mls': 2,\n",
    "           'ssfc':3}\n",
    "            \n",
    "data1_df['sub'] = data1_df['sub'].map(sub_map).astype(int)\n",
    "\n",
    "#create our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "# create the steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('mnb', MultinomialNB())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# init holding lists\n",
    "vec_list = []\n",
    "set_list = []\n",
    "score_list = []\n",
    "    \n",
    "# Fit Train\n",
    "pipe.fit(X_train, y_train)\n",
    "    \n",
    "# Score Train\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('train')\n",
    "score_list.append(pipe.score(X_train, y_train))\n",
    "                      \n",
    "# Score Test\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('test')\n",
    "score_list.append(pipe.score(X_test, y_test)) \n",
    "                 \n",
    "# Fill scores dataframe\n",
    "scores_df = pd.DataFrame(columns = ['vector','set','score'])                      \n",
    "scores_df['vector'] = vec_list\n",
    "scores_df['set'] = set_list\n",
    "scores_df['score'] = score_list\n",
    "    \n",
    "# Create the confusion matrix\n",
    "# Predict Test\n",
    "y_pred = pipe.predict(X_test)\n",
    "    \n",
    "# Confusion Matrix\n",
    "cf = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['mc','fb','mls','ssfc'], index=['mc','fb','mls','ssfc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "Multinomial Bayes did a surprisingly good job compared to the others.\n",
    "\n",
    "The other models are obviously quite overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-5'></a>\n",
    "\n",
    "## KNN\n",
    "\n",
    "Let's try a KNN.\n",
    "\n",
    "Because.\n",
    "\n",
    "I'm going to put it through the standard scaler to make sure everything is nice and tight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([mc_df, fb_df, mls_df, ssfc_df])\n",
    "\n",
    "# instead of one-hot encoding the target apply a map to the 'sub'\n",
    "sub_map = {'mc': 0,\n",
    "           'fb': 1,\n",
    "           'mls': 2,\n",
    "           'ssfc':3}\n",
    "            \n",
    "# Make sure the target is an integer, just to keep it clean\n",
    "data1_df['sub'] = data1_df['sub'].map(sub_map).astype(int)\n",
    "\n",
    "#create our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub']\n",
    "\n",
    "# Init steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('ss', StandardScaler(with_mean=False)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "scores_df, cf, metric_df = vector_scores(X,y,steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['mc','fb','mls','ssfc'], index=['mc','fb','mls','ssfc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "BWHAHAHAHAHA!!!!!\n",
    "\n",
    "Not only is it overfit, it's still really bad! \n",
    "\n",
    "Baseline is .50. So .54 isn't much of an improvement. \n",
    "\n",
    "Worth a shot? I guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thoughts so far.\n",
    "\n",
    "The motorcycle vs. soccer is pretty clear that they are completely different. Makes sense.\n",
    "\n",
    "The rest of them are refinements of the previous set. For example, r/MLS is going to be a subset of what is discussed in r/soccer, and r/SoundersFC is a subset of r/MLS. It is reasonable, therefore, to expect the vocabularies to be a subset as well. This would explain the ease with which the model was able to differentiate the different subreddits, even in a multi-class regression.\n",
    "\n",
    "This leads us to a different experiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-6'></a>\n",
    "\n",
    "## r/TalesFromRetail vs. r/TalesFromYourServer\n",
    "\n",
    "Since the soccer subs were subsets one of the other, I'm going to take two subs that have a similar form: stories. The two subs even have a similar focus: customer service. One is food based, r/TalesFromYourServer, and other is retail based, r/TalesFromRetail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([tfr_df, tfys_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_tfys']\n",
    "\n",
    "# Create the steps\n",
    "steps = [\n",
    "    ('hold', 'hold'),\n",
    "    ('lr', LogisticRegression(solver='liblinear', multi_class='auto' ))]\n",
    "\n",
    "scores_df, cf, metric_df = vector_scores(X,y,steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaiveBayes Multi with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([mls_df, ssfc_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_ssfc']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "# create the steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('mnb', MultinomialNB())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# init holding lists\n",
    "vec_list = []\n",
    "set_list = []\n",
    "score_list = []\n",
    "    \n",
    "# Fit Train\n",
    "pipe.fit(X_train, y_train)\n",
    "    \n",
    "# Score Train\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('train')\n",
    "score_list.append(pipe.score(X_train, y_train))\n",
    "                      \n",
    "# Score Test\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('test')\n",
    "score_list.append(pipe.score(X_test, y_test)) \n",
    "                 \n",
    "# Fill scores dataframe\n",
    "scores_df = pd.DataFrame(columns = ['vector','set','score'])                      \n",
    "scores_df['vector'] = vec_list\n",
    "scores_df['set'] = set_list\n",
    "scores_df['score'] = score_list\n",
    "    \n",
    "# Create the confusion matrix\n",
    "# Predict Test\n",
    "y_pred = pipe.predict(X_test)\n",
    "    \n",
    "# Confusion Matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = cf.ravel()\n",
    "n = tn+fp+fn+tp\n",
    "accuracy = (tp+tn)/n\n",
    "sensitivity = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "specificity = tn/(tn+fp)\n",
    "missclass = (fp+fn)/n\n",
    "value_list = [accuracy, missclass, precision, sensitivity, specificity]\n",
    "metric_df = pd.DataFrame(columns=['metric', 'value'])\n",
    "metric_df.metric = ['Accuracy','Missclassification','Precision','Sensitivity','Specificity']\n",
    "metric_df.value = value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "That was not what I expected. It's still quite overfit, which is, honestly, a bit surprising, but, in keeping with this whole project.\n",
    "\n",
    "Part of this is due to the unbalanced classes. r/TalesFromRetail provided almost twice as many examples as r/TalesFromYourServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-7'></a>\n",
    "\n",
    "## r/TalesFromYourServer vs. r/bartenders\n",
    "\n",
    "Okay, let's try two foodservice subs: r/TalesFromYourServer, r/bartenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([bar_df, tfys_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_tfys']\n",
    "\n",
    "# Create the steps\n",
    "steps = [\n",
    "    ('hold', 'hold'),\n",
    "    ('lr', LogisticRegression(solver='liblinear', multi_class='auto' ))]\n",
    "\n",
    "scores_df, cf, metric_df = vector_scores(X,y,steps)\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaiveBayes Multi with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([mls_df, ssfc_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_ssfc']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "# create the steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('mnb', MultinomialNB())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# init holding lists\n",
    "vec_list = []\n",
    "set_list = []\n",
    "score_list = []\n",
    "    \n",
    "# Fit Train\n",
    "pipe.fit(X_train, y_train)\n",
    "    \n",
    "# Score Train\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('train')\n",
    "score_list.append(pipe.score(X_train, y_train))\n",
    "                      \n",
    "# Score Test\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('test')\n",
    "score_list.append(pipe.score(X_test, y_test)) \n",
    "                 \n",
    "# Fill scores dataframe\n",
    "scores_df = pd.DataFrame(columns = ['vector','set','score'])                      \n",
    "scores_df['vector'] = vec_list\n",
    "scores_df['set'] = set_list\n",
    "scores_df['score'] = score_list\n",
    "    \n",
    "# Create the confusion matrix\n",
    "# Predict Test\n",
    "y_pred = pipe.predict(X_test)\n",
    "    \n",
    "# Confusion Matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = cf.ravel()\n",
    "n = tn+fp+fn+tp\n",
    "accuracy = (tp+tn)/n\n",
    "sensitivity = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "specificity = tn/(tn+fp)\n",
    "missclass = (fp+fn)/n\n",
    "value_list = [accuracy, missclass, precision, sensitivity, specificity]\n",
    "metric_df = pd.DataFrame(columns=['metric', 'value'])\n",
    "metric_df.metric = ['Accuracy','Missclassification','Precision','Sensitivity','Specificity']\n",
    "metric_df.value = value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "FINALLY! I have some issues! I mean the code does. Not that I don't, but...well, I'm not paying ya'll for that kind of help.\n",
    "\n",
    "**ANYWAY!**\n",
    "\n",
    "TO THE BAT-CODE, ROBIN!!!!\n",
    "\n",
    "I'm going to combine the text (post) with the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([bar_df, tfys_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "# Create a series that combines the title and the text\n",
    "t_t = pd.Series(data1_df['title'] + ' ' + data1_df['text'])\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(t_t)\n",
    "y = data1_df['sub_tfys']\n",
    "\n",
    "# Create the steps\n",
    "steps = [\n",
    "    ('hold', 'hold'),\n",
    "    ('lr', LogisticRegression(solver='liblinear', multi_class='auto' ))]\n",
    "\n",
    "scores_df, cf, metric_df = vector_scores(X,y,steps)\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the MNB on the whole shebang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([bar_df, tfys_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "# Create a series that combines the title and the text\n",
    "t_t = pd.Series(data1_df['title'] + ' ' + data1_df['text'])\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(t_t)\n",
    "y = data1_df['sub_tfys']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "# create the steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('mnb', MultinomialNB())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# init holding lists\n",
    "vec_list = []\n",
    "set_list = []\n",
    "score_list = []\n",
    "    \n",
    "# Fit Train\n",
    "pipe.fit(X_train, y_train)\n",
    "    \n",
    "# Score Train\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('train')\n",
    "score_list.append(pipe.score(X_train, y_train))\n",
    "                      \n",
    "# Score Test\n",
    "vec_list.append('CountVector')\n",
    "set_list.append('test')\n",
    "score_list.append(pipe.score(X_test, y_test)) \n",
    "                 \n",
    "# Fill scores dataframe\n",
    "scores_df = pd.DataFrame(columns = ['vector','set','score'])                      \n",
    "scores_df['vector'] = vec_list\n",
    "scores_df['set'] = set_list\n",
    "scores_df['score'] = score_list\n",
    "    \n",
    "# Create the confusion matrix\n",
    "# Predict Test\n",
    "y_pred = pipe.predict(X_test)\n",
    "    \n",
    "# Confusion Matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = cf.ravel()\n",
    "n = tn+fp+fn+tp\n",
    "accuracy = (tp+tn)/n\n",
    "sensitivity = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "specificity = tn/(tn+fp)\n",
    "missclass = (fp+fn)/n\n",
    "value_list = [accuracy, missclass, precision, sensitivity, specificity]\n",
    "metric_df = pd.DataFrame(columns=['metric', 'value'])\n",
    "metric_df.metric = ['Accuracy','Missclassification','Precision','Sensitivity','Specificity']\n",
    "metric_df.value = value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf, columns=['pred neg','pred pos'], index=['actual neg','actual pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That made a huge difference. Interesting, and not that surprising.\n",
    "\n",
    "It's still overfit. So....\n",
    "\n",
    "Let's play around with GridSearchCV to see if it can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-8'></a>\n",
    "\n",
    "## GridSearchCV\n",
    "\n",
    "I'm going to use Gridseach to see if I can get the logistic regression less over-fit.\n",
    "\n",
    "I'll then take that and apply it the first comparison: bikes v balls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from\n",
    "data1_df = pd.concat([bar_df, tfys_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "# Create a series that combines the title and the text\n",
    "t_t = pd.Series(data1_df['title'] + ' ' + data1_df['text'])\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(t_t)\n",
    "y = data1_df['sub_tfys']\n",
    "\n",
    "#typical split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)\n",
    "\n",
    "## Create the steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(multi_class='auto' ))]\n",
    "\n",
    "# Instantiate the Pipe\n",
    "pipe=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Parameter dict\n",
    "params={\n",
    "    'cvec__stop_words':['english'],\n",
    "    'cvec__max_features': [2000],\n",
    "    'cvec__ngram_range':[(1,1)],\n",
    "    'cvec__max_df': [.5],\n",
    "    'lr__penalty':['l2'],\n",
    "    'lr__solver':['lbfgs'],\n",
    "    'lr__max_iter':[10]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = params, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best run found:\n",
    "\n",
    "0.8882030178326474\n",
    "\n",
    "{'cvec__max_df': 0.5, 'cvec__max_features': 2000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'lr__max_iter': 10, 'lr__penalty': 'l2', 'lr__solver': 'lbfgs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train: .8882\n",
    "\n",
    "Test: .9012\n",
    "\n",
    "And that, all in all, isn't too bad.\n",
    "\n",
    "Let's take the same parameters and run it on the original dataset, motorcyce v soccer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from r/motorcycle and r/soccer\n",
    "data1_df = pd.concat([mc_df, fb_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_mc']\n",
    "\n",
    "#typical split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the steps\n",
    "steps = [\n",
    "    ('cvec', CountVectorizer(stop_words='english', max_features=2000, ngram_range=(1,10), max_df=.5)),\n",
    "    ('lr', LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=10 ))]\n",
    "\n",
    "# Instantiate the Pipe\n",
    "pipe=Pipeline(steps)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(f'Train: {pipe.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's interesting. The same models obviously don't apply.\n",
    "\n",
    "Let's go play with Grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combined df to get our X and y from r/motorcycle and r/soccer\n",
    "data1_df = pd.concat([mc_df, fb_df])\n",
    "\n",
    "#hot one encode the sub for our target\n",
    "data1_df = pd.get_dummies(data1_df, columns=['sub'],drop_first='True')\n",
    "\n",
    "#creat our X matrix and y target\n",
    "X = string_clean(data1_df['title'])\n",
    "y = data1_df['sub_mc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Parameter dict\n",
    "mcstop=['motorcycle','motorcycles']\n",
    "params={\n",
    "    'cvec__stop_words':[None],\n",
    "    'cvec__max_features': [3000],\n",
    "    'cvec__ngram_range':[(1,1)],\n",
    "    'cvec__max_df': np.linspace(.01, 1, 20),\n",
    "    'lr__penalty':['l2'],\n",
    "    'lr__solver':['sag'],\n",
    "    'lr__max_iter':[100]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid = params, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 1 candidates, totalling 3 fits\n",
    "\n",
    "0.9318181818181818\n",
    "\n",
    "{'cvec__max_df': 0.06210526315789474, 'cvec__max_features': 3000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': None, 'lr__max_iter': 100, 'lr__penalty': 'l2', 'lr__solver': 'sag'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Pass:\n",
    "\n",
    "vector\tset\tscore\n",
    "- 0\tCountVector\ttrain\t1.000000\n",
    "- 1\tCountVector\ttest\t0.929545\n",
    "- 2\tTfidfVector\ttrain\t0.998485\n",
    "- 3\tTfidfVector\ttest\t0.956818\n",
    "- 4\tHashVector\ttrain\t0.992424\n",
    "- 5\tHashVector\ttest\t0.938636\n",
    "\n",
    "\n",
    "GridSearch\n",
    "\n",
    "CountVector train .9318\n",
    "\n",
    "CountVector test .9318\n",
    "\n",
    "Much better. Interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "Hyperparameters obviously play a large roll in NLP. I can extrapolate from this that they play a large roll pretty much anywhere, and GridSeach() is going to be a valuable tool moving forward.\n",
    "\n",
    "I don't know what it means, but I like the fact that the test dataset did better than the train. That gives me confidence on how well the model will perform on new data.\n",
    "\n",
    "What I find very interesting is the scores being exactly the same from both Test and Train on the motorcycle vs soccer set of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
